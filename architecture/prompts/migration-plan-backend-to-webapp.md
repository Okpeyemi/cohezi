# Plan de Migration : Backend vers Webapp & Streaming AI

Ce document dÃ©taille les Ã©tapes pour fusionner le projet `backend` dans `webapp` et migrer vers le SDK Vercel AI pour activer le streaming.

## ğŸ¯ Objectifs
1.  Supprimer la duplication de code et d'infrastructure (`backend` sÃ©parÃ©).
2.  AmÃ©liorer la latence (suppression du saut rÃ©seau).
3.  Activer le **Streaming** pour un retour utilisateur progressif.

---

## ğŸ“¦ Phase 1 : PrÃ©paration & DÃ©pendances

### 1.1 Installation des paquets dans `webapp`
Nous devons installer les dÃ©pendances nÃ©cessaires pour Gemini et le SDK AI.

```bash
cd webapp
npm install @google/generative-ai ai zod
```

### 1.2 Structure des dossiers
RÃ©organisation recommandÃ©e des fichiers pour une architecture Next.js propre ("Feature-based" ou "Layer-based") :

```text
webapp/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ analyze/       <-- Nouvelle Route API (Streaming)
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ai/               <-- Logique IA (ex-backend/services)
â”‚   â”‚   â”œâ”€â”€ gemini.ts     <-- Client Gemini shared (ex-backend/lib)
â”‚   â”‚   â””â”€â”€ orchestration.ts <-- Service d'analyse
â”‚   â””â”€â”€ prompts/          <-- Prompts migrÃ©s pour accÃ¨s runtime
â””â”€â”€ ...
```

---

## ğŸšš Phase 2 : Migration du Code

### 2.1 DÃ©placement des Prompts (Uniquement ceux utilisÃ©s)
Les prompts nÃ©cessaires au fonctionnement de l'application (ex: `orchestrator.md`, `agents.md`, `synthesis.md`) doivent Ãªtre accessibles au runtime.
*   **Action** : Copier **uniquement** les fichiers utilisÃ©s par `AnalysisService` depuis `architecture/prompts` vers `webapp/lib/prompts`.
*   **Fichiers Ã  copier** : `orchestrator.md`, `agents.md`, `synthesis.md`.
*   **Fichiers exclus** : `migration-plan-backend-to-webapp.md` et autres docs d'architecture inutiles au runtime.
*   **Pourquoi** : Pour garantir leur prÃ©sence dans le bundle de production tout en gardant l'application lÃ©gÃ¨re.

### 2.2 Migration de `gemini.ts`
*   Copier `backend/lib/gemini.ts` vers `webapp/lib/ai/gemini.ts`.
*   Mettre Ã  jour les variables d'environnement (`GEMINI_API_KEY`) dans `.env.local` de `webapp`.

### 2.3 Adaptation de `AnalysisService` pour le Streaming (Vercel AI SDK)
C'est le changement majeur. Au lieu de tout attendre (`await Promise.all`), nous allons utiliser `StreamData` du SDK AI pour envoyer l'avancement au frontend.

**Nouveau concept :**
L'analyse se fait en plusieurs Ã©tapes. Le streaming permet d'envoyer des "updates" JSON partiels ou des Ã©vÃ©nements.

**Approche recommandÃ©e avec `ai` SDK :**
Utiliser `createDataStreamResponse` (Next.js 15+ / AI SDK 4) ou `StreamData` (AI SDK 3).

```typescript
// Exemple conceptuel pour webapp/app/api/analyze/route.ts
import { streamObject, streamText, StreamData } from 'ai';
import { google } from '@ai-sdk/google'; // Si on utilise le provider Google officiel du SDK AI

export async function POST(req: Request) {
  const { decision, reasoning } = await req.json();
  const data = new StreamData();

  // 1. DÃ©marrer le stream immÃ©diatement
  data.append({ status: 'starting', message: 'Analyse initiale...' });

  // Lancer le traitement en asynchrone (non-bloquant pour le premier octet)
  (async () => {
    try {
      // Phase 1: Orchestration
      const orchestrator = await callOrchestrator(decision); 
      data.append({ status: 'orchestration', result: orchestrator });

      // Phase 2: Agents ParallÃ¨les
      const agents = await runParallelAgents(orchestrator);
      data.append({ status: 'agents', count: agents.length });

      // Phase 3: SynthÃ¨se (Streaming du texte final ?)
      // Ici on peut streamer le texte de la synthÃ¨se directement
      const synthesis = await generateSynthesis(orchestrator, agents);
      data.append({ status: 'complete', synthesis });
    } catch (e) {
      data.append({ status: 'error', error: e.message });
    } finally {
      await data.close();
    }
  })();

  return data.toResponse();
}
```

*Alternative plus simple (Migration isochrone)* :
Garder `AnalysisService` tel quel au dÃ©but, et le transformer en *Server Action*.

---

## ğŸ”„ Phase 3 : Mise Ã  jour du Frontend (`webapp`)

### 3.1 Utilisation de `useCompletion` ou `useChat`
Dans `ConclusionModal.tsx` ou `InputPanel.tsx` :

```typescript
import { useCompletion } from 'ai/react';

const { complete, completion, isLoading, data } = useCompletion({
  api: '/api/analyze',
  onFinish: (prompt, result) => {
    // Mise Ã  jour du state final
  }
});

// `data` contiendra les mises Ã  jour en temps rÃ©el (ex: "Analyse logique terminÃ©e...")
// `completion` contiendra le texte streamÃ© s'il y en a.
```

---

## âœ… Checklist d'exÃ©cution

- [ ] **DÃ©pendances** : Installer `@google/generative-ai ai zod` dans `webapp`.
- [ ] **Fichiers** : DÃ©placer `lib/gemini.ts` et `utils/prompt-loader.ts`.
- [ ] **Prompts** : DÃ©placer les fichiers `.md` dans `webapp/prompts`.
- [ ] **Service** : Refactoriser `AnalysisService` pour accepter un `StreamData` writer (optionnel) ou simplement retourner l'objet.
- [ ] **API** : CrÃ©er `app/api/analyze/route.ts`.
- [ ] **Frontend** : Brancher le composant UI sur la nouvelle API.
- [ ] **Nettoyage** : Supprimer le dossier `backend`.

---

## âš ï¸ Points d'attention
*   **Google Provider** : Le SDK Vercel AI a un provider spÃ©cifique `@ai-sdk/google`. Il est souvent plus simple Ã  utiliser pour le streaming que le package `@google/generative-ai` brut, mais ton code actuel utilise le package brut.
    *   *Conseil* : Dans un premier temps, garde ton implÃ©mentation `gemini.ts` actuelle pour ne pas tout casser. Tu peux streamer des donnÃ©es manuelles autour.
    *   *Futur* : Migrer vers `@ai-sdk/google` pour bÃ©nÃ©ficier du `streamObject` natif.
